\section{Empirical}
In this section, we conduct experiments to study the feature selection performances of several well known methods such as Symmetrical Uncertainty Rank, Gain Ratio Rank, Information Gain Rank, Correlation Based Rank, Conditional Entropy Rank \cite{guyon_jmlr03}, Correlation-based Feature Subset Selection \cite{Hall1998},  Reliff \cite{Robnik-Sikonja2003} , mRMR \cite{peng2005} and our proposed methods: High Precision Expectation, High Precision Log likelihood, High Recall Expectation and High Recall Log likelihood. The first six methods are variable rank methods and the last six are subset rank methods. 

In these experimental study, we evaluated each feature selection method in binary classification problems such as Breast Cancer, Pima Diabetes, Heart Statlog, Spect and Vote. These data sets can be downloaded from UCI machine learning repository \cite{Bache+Lichman:2013}. In addition, we used three different classifiers to solve each of the binary classifier problem. Logistic Regression, SVM Linear and Naive Bayes were used in the experiments and the first two are implemented in LibLinear library \cite{REF08a} and the last one is implemented in Weka software \cite{weka}.
 
To evaluate the feature selection performances we reported classification metrics performing 10-fold cross-validation from one until the max number of features selected for each classifier solving each of the binary classification problem. Specifically, we reported Accuracy, Precision, Recall and F-Score for the true label class, that could be identified by the context of the problem and usually be the class with least samples in the data set.