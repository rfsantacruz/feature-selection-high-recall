%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[tbp!]
%\vspace{-1mm}
\centering
{\small 
\begin{tabular}{|l|r|r|r|r|l|} \hline
Dataset & \# Features & \# Data & \# Features/\# Data & \% True Labels & Feature Type\\ \hline \hline 
Breast Cancer (BC)	& 9	& 699	& 0.013	& 34 \%	& All Numeric\\ \hline
Diabetes (D)        & 8	& 768	& 0.010	& 35 \%	& All Numeric\\ \hline
Heart Statlog (HS)	& 13	& 270	& 0.048	& 44 \%	& All Numeric\\ \hline
Spect (S)	        & 22	& 80	& 0.275 & 33 \%	& Categorical\\ \hline
Vote (V)	        & 16	& 435	& 0.037	& 39 \%	& Categorical\\ \hline
Newsgroup (NG)	& 500	& 1963	& 0.255	& 49 \%	& All Binary\\ \hline
Horse Colic (HC)	& 22	& 368	& 0.060	& 37 \%	& Categorical (16) and Numeric (6)\\ \hline
Credit-American (CR)	& 15	& 690	& 0.022	& 44 \%	& Categorical (9) and Numeric (6)\\ \hline
Credit-German (GC)	& 20	& 1000	& 0.020 & 30 \%	& Categorical (13) and Numeric (7)\\ \hline
Hepatitis (H)	& 19	& 155	& 0.123	& 21 \%	& Categorical (13) and Numeric (6)\\ \hline
Ionosphere (I)	& 34	& 351	& 0.097	& 36 \%	& All Numeric\\ \hline
%KR-vs-KP	& 36	& 3196	& 0.011	& 48 \%	& All Categorical\\ \hline
%Labor	        & 16	& 57	& 0.281	& 35 \%	& Categorical (8) and Numeric (8)\\ \hline
%Mushroom	& 22	& 8124	& 0.003	& 48 \%	& All Categorical\\ \hline
%Sick	        & 29	& 3772	& 0.008	&  6 \%	& Categorical (23) and Numeric (6)\\ \hline
%Sonar	        & 60	& 208	& 0.288 & 47 \%	& All Numeric\\ \hline
\end{tabular}}
\vspace{-2mm}
\caption{\footnotesize Descriptive statistics of the various datasets evaluated in this work.}
\label{table:stats}
\vspace{-3.5mm}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp!]
\includegraphics[width=0.85\textwidth]{./figures/dataset_perf/Accuracy.pdf}\\
\includegraphics[width=0.85\textwidth]{./figures/dataset_perf/Recall.pdf}
\includegraphics[width=0.10\textwidth]{./figures/dataset_perf/legend.PNG}
\vspace{-2mm}
\caption{\footnotesize Performance of various feature selection algorithms per dataset, averaged across classifier type and each stage of feature selection.  Overall the newly proposed CVL method performs well on Recall on most datasets, while the Recall performance of other feature selection algorithms vary; interestingly the best performance is achieved for datasets a high ratio of features to data indicating that the strong conjunctive voting scheme of CVL may help reduce noise in feature selection for these datasets. }
\label{fig:perf_vs_dataset}
\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp!]
\centering
\includegraphics[width=0.42\textwidth]{./figures/class_perf/Accuracy.pdf}
\includegraphics[width=0.42\textwidth]{./figures/class_perf/Recall.pdf}
\includegraphics[width=0.15\textwidth]{./figures/class_perf/legend.PNG}
\vspace{-6mm}
\caption{\footnotesize Performance of various feature selection algorithms per classifier, averaged across dataset and each stage of feature selection.  DVE, DVL, CVE, and CVL align with the independence assumptions of Naive Bayes and the maximum (conditional) likelihood framework of Naive Bayes and Logistic Regression, which may explain the better excellent performance of these algorithms with these classifiers.}
\label{fig:perf_vs_classifier}
\end{figure*}
%\vspace{-6mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp!]
\centering
\includegraphics[width=0.38\textwidth]{./figures/linegraphs/SpectLogisticRegressionRecall.pdf}
\includegraphics[width=0.38\textwidth]{./figures/linegraphs/SpectNaiveBayesRecall.pdf}\\
\includegraphics[width=0.38\textwidth]{./figures/linegraphs/NewsgroupLogisticRegressionRecall.pdf}
\includegraphics[width=0.38\textwidth]{./figures/linegraphs/NewsgroupNaiveBayesRecall.pdf}
%\includegraphics[width=0.375\textwidth]{./figures/linegraphs/SpectLogisticRegressionRecall2.pdf}
%\includegraphics[width=0.375\textwidth]{./figures/linegraphs/SpectNaiveBayesRecall2.pdf}\\
%\includegraphics[width=0.375\textwidth]{./figures/linegraphs/NewsgroupLogisticRegressionRecall2.pdf}
%\includegraphics[width=0.375\textwidth]{./figures/linegraphs/NewsgroupNaiveBayesRecall2.pdf}
\caption{\footnotesize Recall performance of various feature selection algorithms for two datasets and classifier types at each stage of feature selection.  The conjunctive voting methods (CVE and CVL)
perform well (and reliably) across all four combinations of classifier and dataset.}
\label{fig:perf_vs_fs_alg}
%\vspace{-2mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Empirical Evaluation}

In this section, we conduct experiments to study the feature selection
performances of several well known methods such as Symmetrical
Uncertainty Rank (SUR), Gain Ratio Rank (GRR), Mutual Information Rank
(MIR), Correlation Based Rank (CR), Conditional Entropy Rank
(CER)~\cite{guyon_jmlr03}, Correlation-based Feature Subset Selection
(CS)~\cite{Hall1998}, Reliff (RF)~\cite{Robnik-Sikonja2003} , Minimum
Redundancy Maximum Relevancem(MRMR)~\cite{peng2005} and our proposed
methods: Disjunction Voting Expectation (DVE) and Disjunction Voting
likelihood (DVL), Conjunction Voting Expectation (CVE), Conjunction
Voting likelihood (CVL).  The first five methods are ranking methods
that select features by ranking them with some metric and the last
seven are subset rank methods that assess subsets of features and
consider previous selections to decide the next selection.  The final
four methods were introduced and derived in the previous section.

In this experimental study, we evaluated each feature selection
method on a variety of binary classification problems 
from the UCI machine learning repository \cite{Bache+Lichman:2013}.
These datasets along with their properties are outlined in 
Table~\ref{table:stats}.

In addition, we used three different classifiers to solve each of the
binary classifier problem. Logistic Regression, SVM Linear and Naive
Bayes were used in the experiments and the first two are implemented
in the LibLinear library~\cite{REF08a} and the last one is implemented in
the Weka software~\cite{weka}.
 
To evaluate feature selection algorithm performance, we perform 
10-fold nested cross-validation (nesting for tuning hyperparameters of each classification
algorithm) and evaluate accuracy, precision, and recall at each stage of
feature selection from the first selected feature through to the maximum
(either 50 or the number of features in the dataset, whichever is smaller).
%and F-Score for the true label class, that
%could be identified by the context of the problem and usually be the
%class with least samples in the data set.

In our evaluation, we wish to answer the following questions:
Do different feature selection methods perform better with
each classifier or on each dataset in comparison to others?
How reliably do the different feature selection methods perform at
different stages of feature selection?  
Do any descriptive dataset features suggest where the different 
algorithms may perform well?

To answer these questions, we first start with
Figure~\ref{fig:perf_vs_dataset}, where we evaluate the performance of
various feature selection algorithms per dataset, averaged across
classifier type and each stage of feature selection. We can observe
here that our approaches generally perform well on recall (at a
reasonable accuracy level).  Notably, CVL performs best and
significantly better than competing methods on the NG and S datasets,
which we remark have the highest feature to data ratio and hence 
represent noisy cases for feature selection.  
%These results 
%indicate that the strong conjunctive voting scheme of CVL may help reduce 
%noise in feature selection for these datasets as previously conjectured.

% TODO Scott

In Figure~\ref{fig:perf_vs_classifier}, we examine how well our 
novel feature selection algorithms DVE, DVL, CVE, CVL 
perform vs. classifier, noting that overall performance is best
for Naive Bayes followed by Logistic Regression and noticeably worse for
the SVM Linear --- it seems that the probabilistic nature of our feature selection
dovetail best with classifiers that are themselves probabilistic,
and overall best with Naive Bayes which makes the same independence assumptions.

Finally, we examine Figure~\ref{fig:perf_vs_fs_alg}, where we evaluate the
performance distribution of various feature selection algorithms for
two datasets -- NG and S -- and classifier types -- Logistic
Regression and Naive Bayes -- at each stage of feature selection.
This graph shows that our CVL and CVE high recall feature selection
approaches provide strong recall performance at all stages of
feature selection in comparison to other selection methods.
%whose performance varies widely.
%We conjecture the reason for this stems from the strong agreement of
%features provided by the conjunctive voting scheme that helps reduce noise
%during feature selection process.

