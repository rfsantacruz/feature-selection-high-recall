\section{Existing Feature Selection Algorithms}

Existing feature selection algorithms fall into categories of filter
methods and wrapper methods \cite{guyon_jmlr03}. Wrapper methods, such as SVM-RFE \cite{guyon2002gene}, select features by evaluating their usefulness to a given classifier, they usually produce the best results but they are computationally expensive to large data sets or to high dimensional problems. Filter methods
evaluate feature subsets according to certain properties of features 
themselves. Within filter methods, feature selection algorithms can be
further classified into ranking methods and subset methods. The former
evaluate each feature independently, while the latter evaluate a
subset at a time \cite{brown2012conditional}. Listed below is a brief
introduction to popular feature selection algorithms that we later
compare to.

Correlation based rank is a na\"{i}ve ranking algorithm that ranks
features according to their linear correlation with the output,
i.e.\ Pearson's $r.$ More sophisticated ranking methods include:
\begin{enumerate}
\item The conditional entropy of class $Y$ given feature $X,$
  i.e.\ $H\left(Y|X\right),$ quantifies the amount of information in
  $Y$ that is not provided by $X.$ Subtracting $H\left(Y|X\right)$
  from the entropy of class $Y,$ the information gain of class $Y$
  given feature $X$ is the mutual information between feature $X$ and
  class $Y,$ i.e.\ $I\left(X;Y\right).$
\item The gain ratio of feature $X$ is defined as the mutual information
  of feature $X$ normalized against the entropy of itself,
  i.e.\ $\frac{I\left(X;Y\right)}{H\left(X\right)}.$
\item The symmetric uncertainty between feature $X$ and class $Y$
  measures the amount of redundancy between them. It is defined as
  $U\left(X,Y\right)=2\frac{I\left(X;Y\right)}{H\left(X\right)+H\left(Y\right)}.$
\item The Relief method \cite{kira1992feature} evaluates the relevance
  of features to the output class according to how well their values
  distinguish between nearest instances of the same and different
  classes.
\end{enumerate}

Correlation-based subset \cite{Hall1998} is an extension
of Pearson's $r$ to subset methods. It measures the linear correlation
between each pair of selected features in addition to the correlation
between selected features and the output class. The other subset
algorithm to which we compared our work is MRMR \cite{peng2005}.
MRMR is an information-theoretic method that not only maximizes
the relevance of selected features to the supervised output class,
but also minimizes the redundancy among selected features. Different
search strategies can be applied using these two algorithms as
heuristics.

Although existing algorithms give various solutions, they are mostly
ad-hoc, until \cite{brown2012conditional}. Brown et al.\ derived a 
probabilistic model with information-theoretic motivations from first principles, and retro-fitted
existing information-theory-based feature selection methods to the
model. In this paper, we also take a derivational approach to
specifying a probabilistic model and optimizing a feature selection
objective w.r.t. that model, but in our case we focus on the specific
task of optimizing for recall in binary classification, which it 
seems no prior method has focused on in previous work.


