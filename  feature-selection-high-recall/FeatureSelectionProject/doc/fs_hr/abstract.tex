Feature selection is important in many practical supervised learning
settings to address computational constraints or to prevent
overfitting when the data may be high-dimensional but contain
relatively few samples.  While a wide variety of feature selection
methods have been proposed in the literature, little research seems to
focus on feature selection specifically targeted to improve recall in
the case of binary classification.  In this paper we propose a novel
probabilistic voting model of feature selection and argue that
choosing features so as to maximize likelihood objectives w.r.t. this
model provides an effective method for high recall feature selection.
For each objective, we derive an efficient formula for feature
selection in the filtering framework (i.e., greedy forward selection)
and we empirically compare the resulting feature selection criteria to
a wide variety of existing methods.  Results show that our high recall
feature selection approach does indeed improve recall with
improvements noticeable when there is a high feature to data ratio.
Such results provide a novel and efficient feature selection algorithm
to target high-recall classification tasks.

%Furthermore, our modeling framework provides us with a way to
%interpret existing feature selection methods and suggests that the
%use of greedy approaches (such as mutual information) appear to
%maximize precision while diversity-based approaches (such as MRMR)
%appear to maximize recall, hence providing us with a framework for
%reinterpreting the loss functions implicitly targeted by existing
%feature selection methods.

% could look at class balance
% what other feature properties?

% may want to mention why P/R important... depends on what is
% important (more costly for false positives or false negatives)
% and accuracy easy to optimize in imbalanced cases
