Notes on submission in fs_hr as of 2/6/2014:

- A quick derivational point... in lines 3 and 4 of the derivation leading to eq 1, there is no need to  convert the conditional from line 2 into its joint definition in line 3... reason... we've conditioned on \vec{x}^d and F^k which are always observed in this model and can simply remain on the conditional side.  In fact the full joint is undefined since we have not defined priors on \vec{x}^d and f_i.

Note that the CIKM-11 derivation was different because we conditioned on some of the latent variables, which are not observed so we had to rewrite that query in terms of the joint distribution given by the graphical model and this rewriting introduced a normalizer that we then had to argue was D-separated from s_k and hence a constant.

- Our graphical model defines a voting-type Naive Bayesian classifier.  At first looking at the conjunctive voting case, I thought we had simply rederived a version of the Naive Bayes classifier.  But if you look closely, the class prior is missing and the conditionals are reversed (we use P(y_i|f_i^x) while NB uses P(f_i^x|y_i)).  Our classifier is also asymmetric in terms of class label... while training NB with flipped labels would not affect NB, flipping labels would affect our algorithm since one class gets a conjunctive vote and the other a disjunctive vote.  This asymmetry allows us to bias our approach towards one class -- this is why it is useful for focusing on precision or recall (which are definitions specific to a specific class, usually the "true" class).  

- For use as an actual classifier, the main parameter to tune would be n for n-out-of-k.  While DP gives us an O(nk) solution to computing log or expected likelihood, I believe there is an even faster exact algorithm that is something like O(k log n).  The idea here is that the recursion can recursively subdivide the features into two equal-sized sets and then combine them by a cross-product of analysis of how the sums in each subdivided set can achieve the required sum in the joint set.

- Since we have a novel efficiently trainable classifier, it is an interesting question as to how it performs in practice compared to NB and other classifiers.  Due to its asymmetry, can it better target precision or recall or high noise cases (*high feature to data ratio*, high noise, high dimensionality)?   I doubt we can beat SVM / LR everywhere, but there may be data cases that are better suited to our algorithm.  This is another paper altogether, but an interesting question worth considering. 

- If one considers that embedded in our graphical model is an actual classifier then in some sense we have actually defined a wrapper approach for this embedded classifier... we repeatedly add features to improve the likelihood of this classifier.  (Training is simple... it's implicit in the P(y_i|f_i^x) so we don't have to explicitly train our model once these conditional probabilities are estimated.)  A natural question though is whether the embedded classifier is what we should actually use for prediction in place of NB, LR, or SVM trained with the features selected by our approach.

- I believe a critical flaw of existing information-theoretic feature selection methods is that they don't take into account the impact of each selected feature on the per-datum classification performance.  Wrapper methods inherently do this (they look at actual performance on the data) and log or expected likelihood is a way to evaluate the performance of a wrapper method.  We have a subset-based wrapper method that is so efficient to evaluate that it can be used as a subset filter method while providing the "data-coverage" benefits of a wrapper approach.

- I've argued in the paper that one reason for our high recall is that we've maximized (expected or log) likelihood.  Likelihood certainly does look at all data.  But I think the answer is a little more complex... it's because we've maximized likelihood w.r.t. an assymetric classifier.  Consider if we had instead used a wrapper method for Naive Bayes and maximized likelihood (it would have the same complexity of our algorithm, just using the Naive Bayes product for each datum rather than the product we derived in each of our conjunctive and disjunctive voting cases)... since Naive Bayes is symmetric w.r.t. all classes, it would not inherently place a preference on the true or false class, which is what we get with our voting classifier by choosing conjunction or disjunction or something in between.

However, since Naive Bayes can be easily used as a subset filter method, in the future, we should implement the few lines of code to evaluate this just to demonstrate that the assymetry in our approach is important for targeting a specific class and that it's not just likelihood but the combination of likelihood with an assymmetric classifier.  This would provide excellent justification for particular graphical model (which we need since the original precision/recall motivation did not quite pan out).

- In a future version of the paper, we should seek out more datasets with a high feature to data ratio (sparse datasets).  If we had more excellent results like Spect and Newsgroup (where we get recall of nearly 1), this paper would be a much easier sell and the boxplots would show the performance that we had hoped to show.  Also we could do a plot where we show recall performance on the x axis vs. different descriptive dataset statistics on the y axis (with each point being a different dataset) and show that for high feature to data ratio datasets, there is a marked advantage in our algorithm (such a result would be clear in such a plot).

- k/2-out-of-k is a symmetric classifier so it cannot target recall over precision (or vice versa).

So I think we have found the "signal" we've been looking for all along!  We just need to strengthen it now that we've better identified the relevant issues and have moved on from our initial assumptions.

